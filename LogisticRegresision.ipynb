{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import woodwork as ww\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This file contains a Logistic Regression model that is used to predict Type 2 Diabetes prevalence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train, validation and test set \n",
    "X =  pd.read_csv('../Processed datasets/After splitting/mrmr/500selected_four_categories.csv')\n",
    "y  = pd.read_csv('../cleaned_imputed_split/y_train.csv')\n",
    "\n",
    "X_val = pd.read_csv('../Processed datasets/After splitting/mrmr/500selected_val_four_categories.csv')\n",
    "y_val = pd.read_csv('../cleaned_imputed_split/y_val.csv')\n",
    "\n",
    "X_test = pd.read_csv('../Processed datasets/After splitting/mrmr/500selected_test_four_categories.csv')\n",
    "y_test = pd.read_csv('../cleaned_imputed_split/y_test.csv')\n",
    "\n",
    "feature_names = X.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 2 by 1 so 0 means non-diabetic and 1 means diabetic \n",
    "y.loc[y['diabetic_outcome'] == 0, 'diabetic_outcome'] = 0\n",
    "y.loc[y['diabetic_outcome'] == 2, 'diabetic_outcome'] = 1\n",
    "\n",
    "y_val.loc[y_val['diabetic_outcome'] == 0, 'diabetic_outcome'] = 0\n",
    "y_val.loc[y_val['diabetic_outcome'] == 2, 'diabetic_outcome'] = 1\n",
    "\n",
    "y_test.loc[y_test['diabetic_outcome'] == 0, 'diabetic_outcome'] = 0\n",
    "y_test.loc[y_test['diabetic_outcome'] == 2, 'diabetic_outcome'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying columns that need rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionary with datatypes \n",
    "import pickle \n",
    "with open('../cleaned_imputed_split/datatype_dictionary.pkl', 'rb') as f:\n",
    "    datatypes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns of datatype Double\n",
    "double = {key: value for key, value in datatypes.items() if value == ww.logical_types.Double}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns of datatype Integer\n",
    "integers = {key: value for key, value in datatypes.items() if value == ww.logical_types.Integer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Doubles and Integers to a list\n",
    "features_to_be_rescaled = list(double.keys()) + list(integers.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of columns that need to be rescaled\n",
    "dataframe_columns = X.columns.tolist()\n",
    "rescaling = []\n",
    "\n",
    "for i in features_to_be_rescaled:\n",
    "    for j in dataframe_columns:\n",
    "        if i in j:\n",
    "            rescaling.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale necessary columns\n",
    "scaler = StandardScaler()\n",
    "X[rescaling] = scaler.fit_transform(X[rescaling])\n",
    "X_val[rescaling] = scaler.fit_transform(X_val[rescaling])\n",
    "X_test[rescaling] = scaler.fit_transform(X_test[rescaling])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate cross-validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert the training and validation data to numpy arrays\n",
    "X = X.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "# Create a empty dictionary to save ROC scores\n",
    "roc_scores = {}\n",
    "\n",
    "# Create a list of number of features to loop over \n",
    "n_features_list = [10, 50, 100, 200]\n",
    "\n",
    "# Fit the LogisticRegression \n",
    "for n_features in n_features_list:\n",
    "    # Wrap model with Recursive Feature Elimination\n",
    "    rfe = RFE(estimator=LogisticRegression(max_iter=5000), n_features_to_select=n_features)\n",
    "    model = LogisticRegression(max_iter=5000)\n",
    "\n",
    "    # Create pipeline with RFE and model\n",
    "    pipeline = Pipeline(steps=[('s', rfe), ('m', model)])\n",
    "\n",
    "    # Fit model to training data \n",
    "    pipeline.fit(X,y.ravel())\n",
    "     \n",
    "    # Evaluate model by performing cross-validation\n",
    "    scores = cross_val_score(pipeline, X,y.ravel(), cv=5, scoring='roc_auc')\n",
    "    \n",
    "    # Print average AUC score \n",
    "    print(scores)\n",
    "    print(\"Average AUC:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate performance of pre-optimized model on the validation set with a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training and validation data to numpy arrays\n",
    "X = X.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "# Train model on train data \n",
    "rfe = RFE(estimator=LogisticRegression(max_iter=5000), n_features_to_select=50)\n",
    "model = LogisticRegression(max_iter=5000)\n",
    "pipeline = Pipeline(steps=[('s', rfe), ('m', model)])\n",
    "pipeline.fit(X,y.ravel())\n",
    "\n",
    "selected_f = [feature_names[i] for i, support in enumerate(rfe.support_) if support]\n",
    "print(selected_f)\n",
    "    \n",
    "# Evaluate model on validation data \n",
    "y_pred_val = pipeline.predict(X_val)\n",
    "\n",
    "# Print AUC score on validation data \n",
    "y_prob_val = pipeline.predict_proba(X_val)[:,1]\n",
    "auc = roc_auc_score(y_val, y_prob_val)\n",
    "print(\"AUC:\",auc)\n",
    "\n",
    "# Print confusion matrix achieved on validation data \n",
    "cm = confusion_matrix(y_val, y_pred_val, labels=[0,1])\n",
    "sns.heatmap(cm, annot=True, fmt='g', xticklabels=['No diabetes', 'Diabetes'], yticklabels=['No diabetes', 'Diabetes'])\n",
    "plt.ylabel('Actual', fontsize=13)\n",
    "plt.xlabel('Predicted', fontsize=13)\n",
    "plt.title('Confusion Matrix', fontsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Select 50 features using RFE\n",
    "rfe = RFE(estimator=LogisticRegression(max_iter=5000), n_features_to_select=50)\n",
    "# Create a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=5000)\n",
    "pipeline = Pipeline(steps=[('s', rfe), ('m', model)])\n",
    "\n",
    "# Fit the pipeline model to the train data\n",
    "pipeline.fit(X,y.ravel())\n",
    "\n",
    "# Define a parameter grid for cross-validation grid search\n",
    "param_grid = {'m__penalty':['l2'], # Regularization penalty\n",
    "              'm__C':[0.01, 0.1, 1.0], # Regularization strength\n",
    "              'm__solver':['lbfgs', 'liblinear', 'newton-cg', 'saga', 'sag'], # Solvers for optimization\n",
    "              's__n_features_to_select':[10, 50, 100, 200]} # Number of features to select using RFE\n",
    "\n",
    "# Perform grid search with cross-validation (cv=5) using ROC AUC as the scoring metric\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc', verbose=1)\n",
    "grid_search.fit(X,y.ravel())\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Convert grid search results to a DataFrame\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Create a pivot table to reorganize the grid search results for visualization\n",
    "pivot_table = results.pivot_table(\n",
    "    values='mean_test_score',\n",
    "    index=['param_m__penalty', 'param_m__C'],\n",
    "    columns=['param_m__solver', 'param_s__n_features_to_select']\n",
    ")\n",
    "\n",
    "\n",
    "# Plot the grid search results in a clustermap\n",
    "sns.clustermap(pivot_table, annot=True, cmap='rocket_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustergrid = sns.clustermap(pivot_table, annot=True, cmap='rocket_r')\n",
    "clustergrid.fig.savefig('hyperparameter_tuning_LR.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate StratifiedKFoldCross results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "# Convert the training and validation data to numpy arrays\n",
    "X = X.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "# Select 50 features using RFE\n",
    "rfe = RFE(estimator=LogisticRegression(max_iter=5000), n_features_to_select=50)\n",
    "# Create a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=5000)\n",
    "pipeline = Pipeline(steps=[('s', rfe), ('m', model)])\n",
    "\n",
    "# Split the data into 5 folds \n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# List to store AUC scores for each fold\n",
    "auc_scores = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in skf.split(X,y):\n",
    "    # Split the data into training and testing sets for the current fold\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit the pipeline model on the training data\n",
    "    pipeline.fit(X_train, y_train.ravel())\n",
    "\n",
    "    # Predict probabilities on the test data of the fold \n",
    "    probas_ = pipeline.predict_proba(X_test)[:,1]\n",
    "\n",
    "     # Calculate the ROC-AUC score for the current fold\n",
    "    auc_score = roc_auc_score(y_test, probas_)\n",
    "\n",
    "    # Append the AUC score to the list\n",
    "    auc_scores.append(auc_score)\n",
    "\n",
    "# Print average AUC score across 5 folds \n",
    "print(auc_scores)\n",
    "average_auc = np.mean(auc_scores)\n",
    "print(f'Average ROC-AUC Score: {average_auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train, validation and test set \n",
    "X =  pd.read_csv('../Processed datasets/After splitting/mrmr/300selected_four_categories.csv')\n",
    "y  = pd.read_csv('../cleaned_imputed_split/y_train.csv')\n",
    "\n",
    "X_val = pd.read_csv('../Processed datasets/After splitting/mrmr/300selected_val_four_categories.csv')\n",
    "y_val = pd.read_csv('../cleaned_imputed_split/y_val.csv')\n",
    "\n",
    "X_test = pd.read_csv('../Processed datasets/After splitting/mrmr/500selected_test_four_categories.csv')\n",
    "y_test = pd.read_csv('../cleaned_imputed_split/y_test.csv')\n",
    "\n",
    "feature_names = X.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 2 by 1 so 0 means non-diabetic and 1 means diabetic \n",
    "y.loc[y['diabetic_outcome'] == 0, 'diabetic_outcome'] = 0\n",
    "y.loc[y['diabetic_outcome'] == 2, 'diabetic_outcome'] = 1\n",
    "\n",
    "y_val.loc[y_val['diabetic_outcome'] == 0, 'diabetic_outcome'] = 0\n",
    "y_val.loc[y_val['diabetic_outcome'] == 2, 'diabetic_outcome'] = 1\n",
    "\n",
    "y_test.loc[y_test['diabetic_outcome'] == 0, 'diabetic_outcome'] = 0\n",
    "y_test.loc[y_test['diabetic_outcome'] == 2, 'diabetic_outcome'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[rescaling] = scaler.fit_transform(X[rescaling])\n",
    "X_val[rescaling] = scaler.fit_transform(X_val[rescaling])\n",
    "X_test[rescaling] = scaler.fit_transform(X_test[rescaling])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train and validation set \n",
    "X_train_full = pd.concat([X, X_val], ignore_index=True, axis=0)\n",
    "y_train_full = pd.concat([y, y_val], ignore_index=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_original = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_train_full.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training and testing data to numpy arrays\n",
    "X = X_train_full.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "y = y_train_full.to_numpy()\n",
    "\n",
    "# Select 50 features using RFE\n",
    "rfe = RFE(estimator=LogisticRegression(max_iter=5000), n_features_to_select=50)\n",
    "# Create a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=5000)\n",
    "pipeline = Pipeline(steps=[('s', rfe), ('m', model)])\n",
    "\n",
    "# Fit the pipeline model to the train data\n",
    "pipeline.fit(X,y.ravel())\n",
    "\n",
    "# Get the selected feature names from the RFE support mask\n",
    "selected_f = [feature_names[i] for i, support in enumerate(rfe.support_) if support]\n",
    "print(selected_f)\n",
    "\n",
    "# Predict target for the test data\n",
    "y_pred_test = pipeline.predict(X_test)\n",
    "\n",
    "# Predict probabilities for the positive class (class 1) in the test data\n",
    "y_prob_test = pipeline.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "# Calculate the ROC-AUC score for the test data predictions\n",
    "auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"AUC:\",auc)\n",
    "\n",
    "# Generate and plot the confusion matrix for the test data predictions\n",
    "cm = confusion_matrix(y_test, y_pred_test, labels=[0,1])\n",
    "sns.heatmap(cm, annot=True, fmt='g', xticklabels=['No diabetes', 'Diabetes'], yticklabels=['No diabetes', 'Diabetes'], cmap='rocket_r')\n",
    "plt.ylabel('Actual', fontsize=13)\n",
    "plt.xlabel('Predicted', fontsize=13)\n",
    "plt.title('Confusion Matrix', fontsize=17)\n",
    "plt.show()\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = feature_names\n",
    "\n",
    "# Get the indices of the selected features from RFE\n",
    "selected_indices = np.where(rfe.support_)[0]\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_features = [feature_names[i] for i in selected_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Transform the feature set using the RFE step in the pipeline\n",
    "X_transformed = pipeline.named_steps['s'].transform(X)\n",
    "\n",
    "# Create a SHAP explainer using the trained Logistic Regression model and the transformed feature set\n",
    "explainer = shap.Explainer(pipeline.named_steps['m'], X_transformed)\n",
    "\n",
    "# Calculate SHAP values for the transformed feature set\n",
    "shap_values = explainer(X_transformed)\n",
    "\n",
    "# Create a summary plot of the SHAP values to show the impact of each feature\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,8))\n",
    "shap.summary_plot(shap_values.values, X_transformed, feature_names=selected_features)\n",
    "\n",
    "plt.savefig('SHAP_summary_plot_LR.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison with TNO features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "# Save the 50 selected features by the wrapper LR model\n",
    "with open('../optimized_model_results/50_selected_features_LR.pkl', 'wb') as f:\n",
    "    pickle.dump(selected_f, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 50 selected features by the wrapper LR model\n",
    "with open('../optimized_model_results/50_selected_features_LR.pkl', 'rb') as f:\n",
    "    selected_f = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Whitehall dictionary from files to use\n",
    "with open('../Processed datasets/feature_dictionary.pkl', 'rb') as f:\n",
    "    columns_whitehall = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of transformation primitives to search for in the feature names\n",
    "trans_primitives = ['SQUARE_ROOT', 'PERCENTILE', 'AND(', 'OR(', 'NOT', '*', '-', '+']\n",
    "\n",
    "# Initialize a dictionary to store counts of features containing each primitive\n",
    "counts = {primitive: [] for primitive in trans_primitives}\n",
    "\n",
    "# Iterate over each transformation primitive\n",
    "for primitive in trans_primitives:\n",
    "    # Iterate over each selected feature name\n",
    "    for feature in selected_f:\n",
    "        # If the primitive is found in the feature name, add the feature to the counts dictionary\n",
    "        if primitive in feature:\n",
    "            counts[primitive].append(feature)\n",
    "\n",
    "# Print the number of features containing each transformation primitive\n",
    "for primitive, count in counts.items():\n",
    "    print(f\"Number of strings containing '{primitive}': {len(count)}\")\n",
    "\n",
    "print(f\"The number of raw features is:\", (len(selected_f) - sum(len(count) for count in counts.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Extract the individual column names from a combined feature \n",
    "def extract_columns(feature):\n",
    "    feature = feature.replace(\"AND(\", \"\").replace(\")\", \"\")\n",
    "    return re.split(r'\\s*[\\+\\-\\*,]\\s', feature)\n",
    "\n",
    "# Map each column name to its corresponding category \n",
    "column_to_category = {}\n",
    "for category, columns in columns_whitehall.items():\n",
    "    for column in columns:\n",
    "        column_to_category[column] = category\n",
    "\n",
    "# Count the number of times that combinations occur \n",
    "category_combinations = Counter()\n",
    "\n",
    "for feature in selected_f:\n",
    "    columns = extract_columns(feature)\n",
    "    involved_categories = {column_to_category.get(column) for column in columns if column and column in column_to_category}\n",
    "    involved_categories.discard(None)\n",
    "\n",
    "    category_combinations[tuple(sorted(involved_categories))] +=1\n",
    "\n",
    "# Print the combinations of categories and their counts \n",
    "for combination, count in category_combinations.items():\n",
    "    print(f\"Combination {combination}: {count} feature(s)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
