{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import woodwork as ww\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import imblearn\n",
    "from scipy.stats import f_oneway\n",
    "import statsmodels.api as sm \n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train, validation and test set \n",
    "X =  pd.read_csv('../Processed datasets/After splitting/mrmr/500selected_four_categories.csv')\n",
    "y  = pd.read_csv('../cleaned_imputed_split/y_train.csv')\n",
    "\n",
    "X_val = pd.read_csv('../Processed datasets/After splitting/mrmr/500selected_val_four_categories.csv')\n",
    "y_val = pd.read_csv('../cleaned_imputed_split/y_val.csv')\n",
    "\n",
    "X_test = pd.read_csv('../Processed datasets/After splitting/mrmr/500selected_test_four_categories.csv')\n",
    "y_test = pd.read_csv('../cleaned_imputed_split/y_test.csv')\n",
    "\n",
    "feature_names = X.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 2 by 1 so 0 means non-diabetic and 1 means diabetic \n",
    "y.loc[y['diabetic_outcome'] == 0, 'diabetic_outcome'] = 0\n",
    "y.loc[y['diabetic_outcome'] == 2, 'diabetic_outcome'] = 1\n",
    "\n",
    "y_val.loc[y_val['diabetic_outcome'] == 0, 'diabetic_outcome'] = 0\n",
    "y_val.loc[y_val['diabetic_outcome'] == 2, 'diabetic_outcome'] = 1\n",
    "\n",
    "y_test.loc[y_test['diabetic_outcome'] == 0, 'diabetic_outcome'] = 0\n",
    "y_test.loc[y_test['diabetic_outcome'] == 2, 'diabetic_outcome'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying columns that need rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionary with datatypes \n",
    "import pickle \n",
    "with open('../cleaned_imputed_split/datatype_dictionary.pkl', 'rb') as f:\n",
    "    datatypes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns of datatype Double\n",
    "double = {key: value for key, value in datatypes.items() if value == ww.logical_types.Double}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns of datatype Integer\n",
    "integers = {key: value for key, value in datatypes.items() if value == ww.logical_types.Integer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Doubles and Integers to a list\n",
    "features_to_be_rescaled = list(double.keys()) + list(integers.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of columns that need to be rescaled\n",
    "dataframe_columns = X.columns.tolist()\n",
    "rescaling = []\n",
    "\n",
    "for i in features_to_be_rescaled:\n",
    "    for j in dataframe_columns:\n",
    "        if i in j:\n",
    "            rescaling.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale necessary columns\n",
    "scaler = StandardScaler()\n",
    "X[rescaling] = scaler.fit_transform(X[rescaling])\n",
    "X_val[rescaling] = scaler.fit_transform(X_val[rescaling])\n",
    "X_test[rescaling] = scaler.fit_transform(X_test[rescaling])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate cross-validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBClassifier \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, make_scorer\n",
    "\n",
    "# Convert the training and validation data to numpy arrays\n",
    "X = X.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "# Create an empty dictionary to save ROC scores\n",
    "roc_scores = {}\n",
    "\n",
    "# Create a list of the number of features to loop over \n",
    "n_features_list = [10, 50, 100, 200]\n",
    "\n",
    "# Fit XGBoost classifier \n",
    "for n_features in n_features_list:\n",
    "    # Wrap model with Recursive Feature Elimination\n",
    "    rfe = RFE(estimator=XGBClassifier(), n_features_to_select=n_features)\n",
    "    model = XGBClassifier()\n",
    "\n",
    "    # Create pipeline with RFE and model\n",
    "    pipeline = Pipeline(steps=[('s', rfe), ('m', model)])\n",
    "\n",
    "    # Fit model to training data \n",
    "    pipeline.fit(X,y.ravel())\n",
    "      \n",
    "    # Evaluate model by performing cross-validation  \n",
    "    scores = cross_val_score(pipeline, X,y.ravel(), cv=5, scoring='roc_auc')\n",
    "    \n",
    "    # Print average AUC score \n",
    "    print(scores)\n",
    "    print(\"Average AUC:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate performance of pre-optimized model on the validation set with a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBClassifier \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, make_scorer\n",
    "\n",
    "# Convert the training and validation data to numpy arrays\n",
    "X = X.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "# Select the specified number of features using RFE\n",
    "rfe = RFE(estimator=XGBClassifier(), n_features_to_select=200)\n",
    "\n",
    "# Create an XGBoost classifier model\n",
    "model = XGBClassifier()\n",
    "\n",
    "# Fit the pipeline model to the training data\n",
    "pipeline = Pipeline(steps=[('s', rfe), ('m', model)])\n",
    "pipeline.fit(X,y.ravel())\n",
    "\n",
    "# Get the selected feature names from the RFE support mask\n",
    "selected_f = [feature_names[i] for i, support in enumerate(rfe.support_) if support]\n",
    "print(selected_f)\n",
    "\n",
    "# Predict target for the validation data\n",
    "y_pred_val = pipeline.predict(X_val)\n",
    "\n",
    "# Predict probabilities for the positive class (class 1) in the validation data\n",
    "y_prob_val = pipeline.predict_proba(X_val)[:,1]\n",
    "\n",
    "# Calculate the ROC-AUC score for the validation data predictions\n",
    "auc = roc_auc_score(y_val, y_prob_val)\n",
    "print(\"AUC:\",auc)\n",
    "\n",
    "\n",
    "# Generate and plot the confusion matrix for the validation data predictions\n",
    "cm = confusion_matrix(y_val, y_pred_val, labels=[0,1])\n",
    "sns.heatmap(cm, annot=True, fmt='g', xticklabels=['No diabetes', 'Diabetes'], yticklabels=['No diabetes', 'Diabetes'])\n",
    "plt.ylabel('Actual', fontsize=13)\n",
    "plt.xlabel('Predicted', fontsize=13)\n",
    "plt.title('Confusion Matrix', fontsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, GridSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBClassifier \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Convert the training and validation data to numpy arrays\n",
    "X = X.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "# Select the specified number of features using RFE\n",
    "rfe = RFE(estimator=XGBClassifier(), n_features_to_select=200)\n",
    "\n",
    "# Create an XGBoost classifier model\n",
    "model = XGBClassifier()\n",
    "\n",
    "# Fit the pipeline model to the training data\n",
    "pipeline = Pipeline(steps=[('s', rfe), ('m', model)])\n",
    "\n",
    "# Define a parameter grid for cross validation grid search \n",
    "param_grid = {'m__learning_rate':[0.01, 0.1, 0.3], # Learning rate for the XGBoost model\n",
    "              'm__min_child_weight':[1,3,6], # Minimum child weight\n",
    "              'm__subsample':[0.7,1], # Subsample ratio \n",
    "              's__n_features_to_select':[10,50, 100, 200]} # Number of feature to select using RFE \n",
    "\n",
    "# Perform grid search with cross-validation (cv=5) using ROC AUC as the scoring metric\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc', verbose=1)\n",
    "grid_search.fit(X,y.ravel())\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Convert grid search results to a DataFrame\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Create a pivot table to reorganize the grid search results for visualization\n",
    "pivot_table = results.pivot_table(\n",
    "    values='mean_test_score',\n",
    "    index=['param_m__subsample', 'param_m__min_child_weight'],\n",
    "    columns=['param_m__learning_rate', 'param_s__n_features_to_select']\n",
    ")\n",
    "\n",
    "# Plot the grid search results in a clustermap\n",
    "sns.clustermap(pivot_table, annot=True, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustergrid = sns.clustermap(pivot_table, annot=True, cmap='rocket_r')\n",
    "clustergrid.fig.savefig('hyperparameter_tuning_XGBoost.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate StratifiedKFoldCross results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBClassifier \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "# Convert the training and validation data to numpy arrays\n",
    "X = X.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "# Select the specified number of features using RFE\n",
    "rfe = RFE(estimator=XGBClassifier(), n_features_to_select=100)\n",
    "\n",
    "# Create an XGBoost classifier model\n",
    "model = XGBClassifier(learning_rate=0.1)\n",
    "\n",
    "# Fit the pipeline model to the training data\n",
    "pipeline = Pipeline(steps=[('s', rfe), ('m', model)])\n",
    "\n",
    "# Split the data into 5 folds \n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "auc_scores = []\n",
    "\n",
    "for train_index, test_index in skf.split(X,y):\n",
    "    # Split the data into training and testing sets for the current fold\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit the pipeline model on the training data\n",
    "    pipeline.fit(X_train, y_train.ravel())\n",
    "\n",
    "    # Predict probabilities for the positive class (class 1) on the test data\n",
    "    probas_ = pipeline.predict_proba(X_test)[:,1]\n",
    "\n",
    "    # Calculate the ROC-AUC score for the current fold\n",
    "    auc_score = roc_auc_score(y_test, probas_)\n",
    "\n",
    "    # Append the AUC score to the list\n",
    "    auc_scores.append(auc_score)\n",
    "\n",
    "# Print average AUC score across 5 folds \n",
    "print(auc_scores)\n",
    "average_auc = np.mean(auc_scores)\n",
    "print(f'Average ROC-AUC Score: {average_auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train, validation and test set \n",
    "X =  pd.read_csv('../Processed datasets/After splitting/mrmr/300selected_four_categories.csv')\n",
    "y  = pd.read_csv('../cleaned_imputed_split/y_train.csv')\n",
    "\n",
    "X_val = pd.read_csv('../Processed datasets/After splitting/mrmr/300selected_val_four_categories.csv')\n",
    "y_val = pd.read_csv('../cleaned_imputed_split/y_val.csv')\n",
    "\n",
    "X_test = pd.read_csv('../Processed datasets/After splitting/mrmr/500selected_test_four_categories.csv')\n",
    "y_test = pd.read_csv('../cleaned_imputed_split/y_test.csv')\n",
    "\n",
    "feature_names = X.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 2 by 1 so 0 means non-diabetic and 1 means diabetic \n",
    "y.loc[y['diabetic_outcome'] == 0, 'diabetic_outcome'] = 0\n",
    "y.loc[y['diabetic_outcome'] == 2, 'diabetic_outcome'] = 1\n",
    "\n",
    "y_val.loc[y_val['diabetic_outcome'] == 0, 'diabetic_outcome'] = 0\n",
    "y_val.loc[y_val['diabetic_outcome'] == 2, 'diabetic_outcome'] = 1\n",
    "\n",
    "y_test.loc[y_test['diabetic_outcome'] == 0, 'diabetic_outcome'] = 0\n",
    "y_test.loc[y_test['diabetic_outcome'] == 2, 'diabetic_outcome'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[rescaling] = scaler.fit_transform(X[rescaling])\n",
    "X_val[rescaling] = scaler.fit_transform(X_val[rescaling])\n",
    "X_test[rescaling] = scaler.fit_transform(X_test[rescaling])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_original = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train and validation set \n",
    "X_train_full = pd.concat([X, X_val], ignore_index=True, axis=0)\n",
    "y_train_full = pd.concat([y, y_val], ignore_index=True, axis=0)\n",
    "\n",
    "# Convert the training and validation data to numpy arrays\n",
    "X = X_train_full.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "y = y_train_full.to_numpy()\n",
    "\n",
    "# Select the specified number of features using RFE\n",
    "rfe = RFE(estimator=XGBClassifier(), n_features_to_select=100)\n",
    "\n",
    "# Create an XGBoost classifier model\n",
    "model = XGBClassifier(learning_rate=0.1)\n",
    "\n",
    "# Fit the pipeline model to the training data\n",
    "pipeline = Pipeline(steps=[('s', rfe), ('m', model)])\n",
    "pipeline.fit(X,y.ravel())\n",
    "\n",
    "# Get the selected feature names from the RFE support mask\n",
    "selected_f = [feature_names[i] for i, support in enumerate(rfe.support_) if support]\n",
    "print(selected_f)\n",
    "\n",
    "# Predict target for the test data\n",
    "y_pred_test = pipeline.predict(X_test)\n",
    "\n",
    "# Predict probabilities for the positive class (class 1) in the test data\n",
    "y_prob_test = pipeline.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calculate the ROC-AUC score for the test data predictions\n",
    "auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"AUC:\",auc)\n",
    "\n",
    "# Generate and plot the confusion matrix for the test data predictions\n",
    "cm = confusion_matrix(y_test, y_pred_test, labels=[0,1])\n",
    "sns.heatmap(cm, annot=True, fmt='g', xticklabels=['No diabetes', 'Diabetes'], yticklabels=['No diabetes', 'Diabetes'], cmap='Blues')\n",
    "plt.ylabel('Actual', fontsize=13)\n",
    "plt.xlabel('Predicted', fontsize=13)\n",
    "plt.title('Confusion Matrix', fontsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed = pipeline.named_steps['s'].transform(X_test)\n",
    "\n",
    "explainer = shap.Explainer(pipeline.named_steps['m'], X_test_transformed)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer(X_test_transformed)\n",
    "\n",
    "# This plot shows the importance of each feature and its impact on the model's predictions\n",
    "shap.summary_plot(shap_values, X_test_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with features Engineered by TNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "# Save the 100 selected features by the wrapped XGBoost model\n",
    "with open('../optimized_model_results/100_selected_features_XGBoost.pkl', 'wb') as f:\n",
    "    pickle.dump(selected_f, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Whitehall dictionary from files to use\n",
    "with open('../Processed datasets/feature_dictionary.pkl', 'rb') as f:\n",
    "    columns_whitehall = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 100 selected features from files to use\n",
    "with open('../optimized_model_results/100_selected_features_XGBoost.pkl', 'rb') as f:\n",
    "    XGBoost_selected_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the features that were inluded by TNO \n",
    "selected_variables_TNO = pd.ExcelFile('../TNO_datasets/Selected Variables.xlsx')  \n",
    "predictors = pd.read_excel(selected_variables_TNO, 'Predictors')\n",
    "predictors_phase_9 = predictors.dropna(subset=['Phase 9'])\n",
    "feature_names_TNO = predictors_phase_9['Phase 9'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the matching features\n",
    "found_features = []\n",
    "\n",
    "# Iterate over each feature name in the feature_names_TNO list\n",
    "for feature in feature_names_TNO:\n",
    "    # Check if the feature is part of any of the XGBoost selected features\n",
    "    if any(feature in combination for combination in XGBoost_selected_features):\n",
    "        # If found, append the feature to the found_features list\n",
    "        found_features.append(feature)\n",
    "\n",
    "count = len(found_features)\n",
    "\n",
    "print(found_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of transformation primitives to search for in the feature names\n",
    "trans_primitives = ['SQUARE_ROOT', 'PERCENTILE', 'AND(', 'OR(', 'NOT', '*', '-', '+']\n",
    "\n",
    "# Initialize a dictionary to store counts of features containing each primitive\n",
    "counts = {primitive: [] for primitive in trans_primitives}\n",
    "\n",
    "# Iterate over each transformation primitive\n",
    "for primitive in trans_primitives:\n",
    "    # Iterate over each selected feature name\n",
    "    for feature in selected_f:\n",
    "         # If the primitive is found in the feature name, add the feature to the counts dictionary\n",
    "        if primitive in feature:\n",
    "            counts[primitive].append(feature)\n",
    "\n",
    "# Print the number of features containing each transformation primitive\n",
    "for primitive, count in counts.items():\n",
    "    print(f\"Number of strings containing '{primitive}': {len(count)}\")\n",
    "\n",
    "print(f\"The number of raw features is:\", (len(selected_f) - sum(len(count) for count in counts.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Extract the individual column names from a combined feature \n",
    "def extract_columns(feature):\n",
    "    feature = feature.replace(\"AND(\", \"\").replace(\")\", \"\")\n",
    "    return re.split(r'\\s*[\\+\\-\\*,]\\s', feature)\n",
    "\n",
    "# Map each column name to its corresponding category \n",
    "column_to_category = {}\n",
    "for category, columns in columns_whitehall.items():\n",
    "    for column in columns:\n",
    "        column_to_category[column] = category\n",
    "\n",
    "# Count the number of times that combinations occur \n",
    "category_combinations = Counter()\n",
    "\n",
    "for feature in selected_f:\n",
    "    columns = extract_columns(feature)\n",
    "    involved_categories = {column_to_category.get(column) for column in columns if column and column in column_to_category}\n",
    "    involved_categories.discard(None)\n",
    "\n",
    "    category_combinations[tuple(sorted(involved_categories))] +=1\n",
    "    \n",
    "# Print the combinations of categories and their counts \n",
    "for combination, count in category_combinations.items():\n",
    "    print(f\"Combination {combination}: {count} feature(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../optimized_model_results/50_selected_features_LR.pkl', 'rb') as f:\n",
    "    selected_features_LR = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_set = set(selected_features_XGBoost)\n",
    "LR_set = set(selected_features_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how many selected features XGBoost and LR have in common\n",
    "common_features = XGBoost_set.intersection(LR_set)\n",
    "common_feature_count = len(common_features)\n",
    "common_feature_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In depth error analysis TP, FN, and FP cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train, validation and test set \n",
    "X =  pd.read_csv('../Processed datasets/After splitting/mrmr/500selected_four_categories.csv')\n",
    "y  = pd.read_csv('../cleaned_imputed_split/y_train.csv')\n",
    "\n",
    "X_val = pd.read_csv('../Processed datasets/After splitting/mrmr/500selected_val_four_categories.csv')\n",
    "y_val = pd.read_csv('../cleaned_imputed_split/y_val.csv')\n",
    "\n",
    "X_test = pd.read_csv('../Processed datasets/After splitting/mrmr/500selected_test_four_categories.csv')\n",
    "y_test = pd.read_csv('../cleaned_imputed_split/y_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 2 by 1 so 0 means non-diabetic and 1 means diabetic \n",
    "y.loc[y['diabetic_outcome'] == 0, 'diabetic_outcome'] = 0\n",
    "y.loc[y['diabetic_outcome'] == 2, 'diabetic_outcome'] = 1\n",
    "\n",
    "y_val.loc[y_val['diabetic_outcome'] == 0, 'diabetic_outcome'] = 0\n",
    "y_val.loc[y_val['diabetic_outcome'] == 2, 'diabetic_outcome'] = 1\n",
    "\n",
    "y_test.loc[y_test['diabetic_outcome'] == 0, 'diabetic_outcome'] = 0\n",
    "y_test.loc[y_test['diabetic_outcome'] == 2, 'diabetic_outcome'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[rescaling] = scaler.fit_transform(X[rescaling])\n",
    "X_val[rescaling] = scaler.fit_transform(X_val[rescaling])\n",
    "X_test[rescaling] = scaler.fit_transform(X_test[rescaling])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBClassifier \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, make_scorer\n",
    "# Merge train and validation set \n",
    "X_train_full = pd.concat([X, X_val], ignore_index=True, axis=0)\n",
    "y_train_full = pd.concat([y, y_val], ignore_index=True, axis=0)\n",
    "\n",
    "# Convert the training and validation data to numpy arrays\n",
    "X = X.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "# Select the specified number of features using RFE\n",
    "rfe = RFE(estimator=XGBClassifier(), n_features_to_select=100)\n",
    "\n",
    "# Create an XGBoost classifier model\n",
    "model = XGBClassifier(learning_rate=0.1)\n",
    "\n",
    "# Fit the pipeline model to the training data\n",
    "pipeline = Pipeline(steps=[('s', rfe), ('m', model)])\n",
    "pipeline.fit(X,y.ravel())\n",
    "\n",
    "# Get the selected feature names from the RFE support mask\n",
    "selected_f = [feature_names[i] for i, support in enumerate(rfe.support_) if support]\n",
    "print(selected_f)\n",
    "\n",
    "# Predict target for the test data\n",
    "y_pred_test = pipeline.predict(X_test)\n",
    "\n",
    "# Predict probabilities for the positive class (class 1) in the test data\n",
    "y_prob_test = pipeline.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calculate the ROC-AUC score for the test data predictions\n",
    "auc = roc_auc_score(y_test, y_prob_test)\n",
    "print(\"AUC:\",auc)\n",
    "\n",
    "# Generate and plot the confusion matrix for the test data predictions\n",
    "cm = confusion_matrix(y_test, y_pred_test, labels=[0,1])\n",
    "sns.heatmap(cm, annot=True, fmt='g', xticklabels=['No diabetes', 'Diabetes'], yticklabels=['No diabetes', 'Diabetes'], cmap='Blues')\n",
    "plt.ylabel('Actual', fontsize=13)\n",
    "plt.xlabel('Predicted', fontsize=13)\n",
    "plt.title('Confusion Matrix', fontsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the test feature set (X_test) with columns named after the features\n",
    "X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
    "\n",
    "# Select only the features that were selected by RFE\n",
    "X_test_df = X_test_df[selected_f]\n",
    "\n",
    "# Add a column to the DataFrame with the predicted labels for the test data\n",
    "X_test_df['predicted'] = y_pred_test\n",
    "\n",
    "# Add a column to the DataFrame with the actual labels for the test data\n",
    "X_test_df['actual'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define conditions for different prediction outcomes\n",
    "# (True Positive, False Positive, False Negative, True Negative)\n",
    "conditions = [(X_test_df['predicted'] == 1) & (X_test_df['actual'] == 1),\n",
    "              (X_test_df['predicted'] == 1) & (X_test_df['actual'] == 0),\n",
    "              (X_test_df['predicted'] == 0) & (X_test_df['actual'] == 1),\n",
    "              (X_test_df['predicted'] == 0) & (X_test_df['actual'] == 0)]\n",
    "\n",
    "# Define labels corresponding to each condition\n",
    "# 1: True Positive (TP)\n",
    "# 2: False Positive (FP)\n",
    "# 3: False Negative (FN)\n",
    "# 4: True Negative (TN)\n",
    "\n",
    "labels = [1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'confusion_matrix_category' in the test DataFrame\n",
    "# This column will categorize each prediction based on the conditions defined earlier\n",
    "# np.select applies the conditions in the given order and assigns the corresponding label\n",
    "# If none of the conditions are met, the default value (np.nan) is assigned\n",
    "X_test_df['confusion_matrix_category'] = np.select(conditions, labels, default=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop True Negatives\n",
    "X_test_df = X_test_df[X_test_df['confusion_matrix_category'] != 4]\n",
    "\n",
    "# Drop created columns with the predicted and actual labels \n",
    "X_test_df.drop(columns=['predicted', 'actual'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the ANOVA results\n",
    "results = {}\n",
    "\n",
    "# Iterate over each feature column in the test DataFrame (excluding the last column 'confusion_matrix_category')\n",
    "# Determine if there are statistically significant differences in feature values between the groups defined in the column 'confusion_matrix_category'\n",
    "for feature in X_test_df.columns[:-1]:\n",
    "    model = ols(f'Q(\"{feature}\") ~ C(confusion_matrix_category)', data = X_test_df).fit()\n",
    "\n",
    "    # Perform ANOVA (Analysis of Variance) on the OLS model\n",
    "    anova_result = sm.stats.anova_lm(model, typ=2)\n",
    "    p_value = anova_result.at['C(confusion_matrix_category)', 'PR(>F)']\n",
    "    results[feature] = p_value\n",
    "\n",
    "# Set p < 0.05 to print statistically different features \n",
    "for feature, p_val in results.items():\n",
    "    if p_val < 0.05:\n",
    "        print(f\"{feature}: p-value = {p_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplot of top predictor \n",
    "sns.boxplot(x='confusion_matrix_category', y='JCHOLMED * JP10CVD', data=X_test_df)\n",
    "plt.title('JCHOLDMED * JP10CVD boxplot per category')\n",
    "plt.xlabel('Category')\n",
    "group_labels = ['True positive', 'False positive', 'False negative']\n",
    "plt.xticks(ticks=plt.gca().get_xticks(), labels=group_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplot of top predictor \n",
    "sns.boxplot(x='confusion_matrix_category', y='JGLUC_2H * JP10CVD', data=X_test_df)\n",
    "plt.title('JGLUC_2H * JP10CVD boxplot per category')\n",
    "plt.xlabel('Category')\n",
    "group_labels = ['True positive', 'False positive', 'False negative']\n",
    "plt.xticks(ticks=plt.gca().get_xticks(), labels=group_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplot of top predictor \n",
    "sns.boxplot(x='confusion_matrix_category', y='JFAT * JP10CVD', data=X_test_df)\n",
    "plt.title('JFAT * JP10CVD boxplot per category')\n",
    "plt.xlabel('Category')\n",
    "group_labels = ['True positive', 'False positive', 'False negative']\n",
    "plt.xticks(ticks=plt.gca().get_xticks(), labels=group_labels)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
